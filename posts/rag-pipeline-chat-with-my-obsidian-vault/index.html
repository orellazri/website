<!DOCTYPE html>
<html lang="en" class="dark light">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="base" content="https:&#x2F;&#x2F;orellazri.com">

    

    
    
    
    <title>
         RAG Pipeline to Chat with My Obsidian Vault
        
    </title>

        
            <meta property="og:title" content="RAG Pipeline to Chat with My Obsidian Vault" />
        
     

     
         
     

     
         
    

    
    
        <link rel="icon" type="image/png" href=&#x2F;favicon.svg />
    

    
    
        <link href=https://orellazri.com/fonts.css rel="stylesheet" />
    

    
    
        

        
            
            

            <script data-goatcounter="https://orellazri.goatcounter.com/count" async src="https://orellazri.com/js/count.js"></script>
            <noscript>
                
                <img src="https://orellazri.goatcounter.com//count?p=&#x2F;posts&#x2F;rag-pipeline-chat-with-my-obsidian-vault&#x2F;&t=RAG Pipeline to Chat with My Obsidian Vault">
            </noscript>
        
    

    
    

    
    
        <script src=https://orellazri.com/js/toc.js></script>
    

    
    

    

    
    <link rel="alternate" type="application/atom+xml" title="Orel Lazri" href="https://orellazri.com/atom.xml">


    
    
        <link rel="stylesheet" type="text/css" href=https://orellazri.com/theme/light.css />
        <link rel="stylesheet" type="text/css" href="https://orellazri.com/theme/dark.css" media="(prefers-color-scheme: dark)" />
    

    <!-- Set the correct theme in the script -->
    <script src=https://orellazri.com/js/themetoggle.js></script>
    
        <script>
            if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
                setTheme("dark");
            } else {
                setTheme("light");
            }
        </script>
    

    <link rel="stylesheet" type="text/css" media="screen" href=https://orellazri.com/main.css />

    

    <script src=https://orellazri.com/js/mermaid.js></script>

    <script defer src="https://orellazri.com/search_index.en.js?h=3a69812dc9773e8d3635"></script>
        <script defer src="https://orellazri.com/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e"></script></head>


<body>
    <div class="content">
        <header>
    <div class="main">
        
            <a href=https:&#x2F;&#x2F;orellazri.com>Orel Lazri</a>
        


        <div class="socials">
            
            <a rel="me" href="https:&#x2F;&#x2F;github.com&#x2F;orellazri" class="social">
                <img alt=github src=https://orellazri.com/social_icons/github.svg>
            </a>
            
            <a rel="me" href="https:&#x2F;&#x2F;linkedin.com&#x2F;in&#x2F;orellazri" class="social">
                <img alt=linkedin src=https://orellazri.com/social_icons/linkedin.svg>
            </a>
            
        </div>
    </div>

    <nav>
        
            <a href=https://orellazri.com/posts style="margin-left: 0.25em">posts</a>
        
            <a href=https://orellazri.com/projects style="margin-left: 0.25em">projects</a>
        
            <a href=https://orellazri.com/atom.xml style="margin-left: 0.25em">rss</a>
        

        
        <button 
            id="search-button"
            class="search-button"
            title="$SHORTCUT to open search"
        >
            <img 
                src="https://orellazri.com/search.svg" 
                alt="Search" 
                class="search-icon"
            >
        </button>

        <div id="searchModal" class="search-modal js" role="dialog" aria-labelledby="modalTitle">
            <div id="modal-content">
                <h1 id="modalTitle" class="page-header">Search</h1>
                <div id="searchBar">
                    <input 
                        id="searchInput" 
                        role="combobox" 
                        autocomplete="off" 
                        spellcheck="false" 
                        aria-expanded="false" 
                        aria-controls="results-container" 
                        placeholder="Search..."
                    />
                    <button 
                        id="clear-search" 
                        class="clear-button"
                        title="Clear search"
                    >
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 -960 960 960">
                            <path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/>
                        </svg>
                    </button>
                </div>
                <div id="results-container">
                    <div id="results-info">
                        <span id="zero_results" style="display: none;">No results</span>
                        <span id="one_result" style="display: none;">1 result</span>
                        <span id="many_results" style="display: none;">$NUMBER results</span>
                    </div>
                    <div id="results" role="listbox"></div>
                </div>
            </div>
        </div>
        

        
    </nav>
</header>


        
        
    
<main>
    <article>
        <div class="title">
            
            
    <div class="page-header">
        RAG Pipeline to Chat with My Obsidian Vault<span class="primary-color" style="font-size: 1.6em">.</span>
    </div>


                <div class="meta">
                    
                        Posted on <time>2025-06-21</time>
                    

                    

                    

                    
                    
                            <span class="tags-label"> :: Tags:</span>
                            <span class="tags">
                                    <a href="https://orellazri.com/tags/coding/" class="post-tag">coding</a>, 
                                
                                    <a href="https://orellazri.com/tags/ai/" class="post-tag">ai</a>
                                
                            </span>
                    

                    
                    

                    

                </div>
        </div>

        

        
        
        
            <div class="toc-container">
                <h1 class="toc-title">Table of Contents</h1>
                <ul class="toc-list">
                    
                        <li>
                            <a href="https://orellazri.com/posts/rag-pipeline-chat-with-my-obsidian-vault/#a-local-rag-pipeline">A Local RAG Pipeline</a>
                            
                        </li>
                    
                        <li>
                            <a href="https://orellazri.com/posts/rag-pipeline-chat-with-my-obsidian-vault/#the-implementation">The Implementation</a>
                            
                                <ul>
                                    
                                        <li>
                                            <a href="https://orellazri.com/posts/rag-pipeline-chat-with-my-obsidian-vault/#first-step-markdown-parsing-and-chunking">First step: Markdown Parsing and Chunking</a>
                                        </li>

                                        
                                    
                                        <li>
                                            <a href="https://orellazri.com/posts/rag-pipeline-chat-with-my-obsidian-vault/#second-step-vector-embeddings-with-sqlite">Second step: Vector Embeddings with SQLite</a>
                                        </li>

                                        
                                    
                                        <li>
                                            <a href="https://orellazri.com/posts/rag-pipeline-chat-with-my-obsidian-vault/#third-step-querying-the-vector-database">Third step: Querying the vector database</a>
                                        </li>

                                        
                                    
                                </ul>
                            
                        </li>
                    
                        <li>
                            <a href="https://orellazri.com/posts/rag-pipeline-chat-with-my-obsidian-vault/#usage">Usage</a>
                            
                        </li>
                    
                        <li>
                            <a href="https://orellazri.com/posts/rag-pipeline-chat-with-my-obsidian-vault/#local-first-approach">Local-First Approach</a>
                            
                        </li>
                    
                </ul>
            </div>
        
        

        <section class="body">
            <p>My Obsidian vault has grown significantly over time with snippets, project documentation, meeting notes, and random thoughts. I wanted a conversational way to interact with my notes - to ask natural language questions like "How did I fix that Git submodule issue?" or "What were the main challenges in the refactor?" and get contextual answers from my notes.</p>
<p>Rather than relying on cloud-based solutions, I decided to see if I could build my own RAG (Retrieval Augmented Generation) pipeline that runs entirely locally, keeping my notes private and giving me full control over the implementation - mainly for fun.</p>
<p>You can find the source code <a href="https://github.com/orellazri/mdrag">here</a>.</p>
<h2 id="a-local-rag-pipeline"><a class="zola-anchor" href="#a-local-rag-pipeline" aria-label="Anchor link for: a-local-rag-pipeline">A Local RAG Pipeline</a></h2>
<p>I built <strong>mdrag</strong> (Markdown RAG), a Rust-based CLI tool that creates a searchable vector database from markdown files and uses local AI models to generate responses. The name might not be the most creative, but it gets the job done!</p>
<p>The system works in two phases:</p>
<ol>
<li><strong>Embedding</strong>: Process all markdown files, chunk them intelligently, and generate vector embeddings</li>
<li><strong>Querying</strong>: Take natural language queries, find similar content, and use a local LLM to generate answers</li>
</ol>
<h2 id="the-implementation"><a class="zola-anchor" href="#the-implementation" aria-label="Anchor link for: the-implementation">The Implementation</a></h2>
<p>These are the main interesting dependencies I've used:</p>
<ul>
<li><a href="https://docs.rs/rusqlite/latest/rusqlite/">rusqlite</a> and <a href="https://docs.rs/sqlite-vec/latest/sqlite_vec/">sqlite-vec</a> for using SQLite as a vector database</li>
<li><a href="https://docs.rs/ollama-rs/latest/ollama_rs/">ollama-rs</a> - This allows me to interact with a local Ollama instance in an idiomatic way instead of writing and parsing plain requests</li>
<li><a href="https://docs.rs/zerocopy/latest/zerocopy/">zerocopy</a> - For zero-cost memory manipulation. This is used to reference data in and out of SQLite without memory allocations.</li>
</ul>
<h3 id="first-step-markdown-parsing-and-chunking"><a class="zola-anchor" href="#first-step-markdown-parsing-and-chunking" aria-label="Anchor link for: first-step-markdown-parsing-and-chunking">First step: Markdown Parsing and Chunking</a></h3>
<p>One of the trickiest parts was intelligently chunking markdown content. Simply splitting by character count would break apart related information. Instead, I implemented a parser that is slightly more sophisticated, though it could still be improved significantly:</p>
<ol>
<li><strong>Detects the primary header level</strong> used in each document</li>
<li><strong>Splits content by those headers</strong>, keeping related information together</li>
<li><strong>Handles edge cases</strong> like content before the first header</li>
<li><strong>Further splits large chunks</strong> while respecting paragraph boundaries</li>
</ol>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#b48ead;">pub fn </span><span style="color:#8fa1b3;">parse</span><span>(</span><span style="color:#bf616a;">contents</span><span>: &amp;</span><span style="color:#b48ead;">str</span><span>, </span><span style="color:#bf616a;">filename</span><span>: &amp;</span><span style="color:#b48ead;">str</span><span>) -&gt; Result&lt;Vec&lt;String&gt;&gt; {
</span><span>    </span><span style="color:#65737e;">// Find the first header type used in the document
</span><span>    </span><span style="color:#b48ead;">let</span><span> header_regex = Regex::new(</span><span style="color:#b48ead;">r</span><span>&quot;</span><span style="color:#a3be8c;">^(#{1,6})\s</span><span>&quot;)?;
</span><span>
</span><span>    </span><span style="color:#b48ead;">let</span><span> first_header_level = contents
</span><span>        .</span><span style="color:#96b5b4;">lines</span><span>()
</span><span>        .</span><span style="color:#96b5b4;">find_map</span><span>(|</span><span style="color:#bf616a;">line</span><span>| header_regex.</span><span style="color:#96b5b4;">captures</span><span>(line).</span><span style="color:#96b5b4;">map</span><span>(|</span><span style="color:#bf616a;">caps</span><span>| caps[</span><span style="color:#d08770;">1</span><span>].</span><span style="color:#96b5b4;">to_string</span><span>()));
</span><span>
</span><span>    </span><span style="color:#65737e;">// Split by the detected header level or treat as single chunk
</span><span>    </span><span style="color:#65737e;">// Each chunk gets prefixed with the filename for context
</span><span>}
</span></code></pre>
<p>This approach ensures that a section like "## Git Snippets" stays together with all its related commands, making retrieval more meaningful.</p>
<h3 id="second-step-vector-embeddings-with-sqlite"><a class="zola-anchor" href="#second-step-vector-embeddings-with-sqlite" aria-label="Anchor link for: second-step-vector-embeddings-with-sqlite">Second step: Vector Embeddings with SQLite</a></h3>
<p>Instead of using a dedicated vector database, I leveraged <strong>sqlite-vec</strong>, a SQLite extension that adds vector similarity search capabilities. This keeps the entire system self-contained-no additional services to run or manage.</p>
<p>This extension can be loaded with a short unsafe block:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#b48ead;">unsafe </span><span>{
</span><span>    </span><span style="color:#96b5b4;">sqlite3_auto_extension</span><span>(Some(std::mem::transmute(sqlite3_vec_init as </span><span style="color:#b48ead;">*const </span><span>())));
</span><span>}
</span></code></pre>
<p>And then we can create a table with the following schema:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#b48ead;">pub fn </span><span style="color:#8fa1b3;">new</span><span>(</span><span style="color:#bf616a;">conn</span><span>: &amp;</span><span style="color:#b48ead;">&#39;a</span><span> Connection, </span><span style="color:#bf616a;">ollama</span><span>: &amp;</span><span style="color:#b48ead;">&#39;a</span><span> Ollama) -&gt; Result&lt;</span><span style="color:#b48ead;">Self</span><span>&gt; {
</span><span>    conn.</span><span style="color:#96b5b4;">execute</span><span>(
</span><span>        &quot;</span><span style="color:#a3be8c;">CREATE VIRTUAL TABLE IF NOT EXISTS file_embeddings USING vec0(
</span><span style="color:#a3be8c;">            path TEXT,
</span><span style="color:#a3be8c;">            contents TEXT,
</span><span style="color:#a3be8c;">            embedding FLOAT[768]
</span><span style="color:#a3be8c;">        )</span><span>&quot;,
</span><span>        [],
</span><span>    )?;
</span><span>
</span><span>    Ok(</span><span style="color:#b48ead;">Self </span><span>{ conn, ollama })
</span><span>}
</span></code></pre>
<p>The embedding process walks through all markdown files in the vault, checks if they've already been processed (to avoid redundant work), and generates 768-dimensional vectors using Ollama's <a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1.5">nomic-embed-text-v1.5</a> model. Using a different model means that the embedding vector size will probably need to be changed.</p>
<h3 id="third-step-querying-the-vector-database"><a class="zola-anchor" href="#third-step-querying-the-vector-database" aria-label="Anchor link for: third-step-querying-the-vector-database">Third step: Querying the vector database</a></h3>
<p>In querying mode, a user enters a natural language query, and the system generates an embedding for it, then uses the vector database to find the most similar chunks. Finally, it uses the LLM to generate a response.</p>
<p>The interesting part where we fetch the similar vectors from SQLite is this:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#b48ead;">let mut</span><span> stmt = </span><span style="color:#bf616a;">self</span><span>.conn.</span><span style="color:#96b5b4;">prepare</span><span>(
</span><span>    &quot;</span><span style="color:#a3be8c;">SELECT contents
</span><span style="color:#a3be8c;">    FROM file_embeddings
</span><span style="color:#a3be8c;">    WHERE embedding MATCH ?1
</span><span style="color:#a3be8c;">    AND k = ?2
</span><span style="color:#a3be8c;">    ORDER BY distance</span><span>&quot;,
</span><span>)?;
</span><span>
</span><span style="color:#b48ead;">let</span><span> results = stmt
</span><span>    .</span><span style="color:#96b5b4;">query_map</span><span>((query_embedding[</span><span style="color:#d08770;">0</span><span>].</span><span style="color:#96b5b4;">as_bytes</span><span>(), k as </span><span style="color:#b48ead;">i32</span><span>), |</span><span style="color:#bf616a;">row</span><span>| {
</span><span>        Ok(row.</span><span style="color:#96b5b4;">get</span><span>(</span><span style="color:#d08770;">0</span><span>)?)
</span><span>    })?
</span><span>    .collect::&lt;Result&lt;Vec&lt;String&gt;, _&gt;&gt;()?;
</span></code></pre>
<p>I used the <code>embedding MATCH</code> clause to find the most similar vectors to the query embedding, and return the contents (the original text that was embedded). This then lets the LLM generate a response based on the context of the most similar chunks.</p>
<p>For the actual text generation, I integrated with <strong>Ollama</strong>, which provides a simple API for running various models locally. I chose <code>gemma3:1b</code> because I wanted an extremely lightweight model to test things quickly.</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#65737e;">// Embed the query
</span><span style="color:#b48ead;">let</span><span> results = embedder.</span><span style="color:#96b5b4;">search</span><span>(&amp;query, </span><span style="color:#d08770;">5</span><span>).await?;
</span><span>
</span><span style="color:#65737e;">// Build a prompt
</span><span style="color:#b48ead;">let</span><span> model = &quot;</span><span style="color:#a3be8c;">gemma3:1b</span><span>&quot;.</span><span style="color:#96b5b4;">to_string</span><span>();
</span><span style="color:#b48ead;">let</span><span> options = ModelOptions::default()
</span><span>    .</span><span style="color:#96b5b4;">temperature</span><span>(</span><span style="color:#d08770;">0.2</span><span>)
</span><span>    .</span><span style="color:#96b5b4;">top_k</span><span>(</span><span style="color:#d08770;">25</span><span>)
</span><span>    .</span><span style="color:#96b5b4;">top_p</span><span>(</span><span style="color:#d08770;">0.25</span><span>);
</span><span style="color:#b48ead;">let mut</span><span> prompt = format!(
</span><span>    &quot;</span><span style="color:#a3be8c;">Answer the following user question: </span><span style="color:#d08770;">{query}</span><span style="color:#96b5b4;">\n\n</span><span style="color:#a3be8c;">Using the following information for context:</span><span style="color:#96b5b4;">\n\n</span><span>&quot;
</span><span>);
</span><span>results
</span><span>    .</span><span style="color:#96b5b4;">iter</span><span>()
</span><span>    .</span><span style="color:#96b5b4;">for_each</span><span>(|</span><span style="color:#bf616a;">result</span><span>| prompt.</span><span style="color:#96b5b4;">push_str</span><span>(&amp;format!(&quot;</span><span style="color:#96b5b4;">\n\n</span><span style="color:#d08770;">{}</span><span>&quot;, result)));
</span><span>
</span><span style="color:#65737e;">// Generate a streaming response
</span><span style="color:#b48ead;">let mut</span><span> stream = ollama
</span><span>    .</span><span style="color:#96b5b4;">generate_stream</span><span>(GenerationRequest::new(model, prompt).</span><span style="color:#96b5b4;">options</span><span>(options))
</span><span>    .await?;
</span><span>
</span><span style="color:#65737e;">// Stream the response to stdout
</span><span style="color:#b48ead;">let mut</span><span> stdout = io::stdout();
</span><span style="color:#b48ead;">while let </span><span>Some(res) = stream.</span><span style="color:#96b5b4;">next</span><span>().await {
</span><span>    </span><span style="color:#b48ead;">let</span><span> responses = res?;
</span><span>    </span><span style="color:#b48ead;">for</span><span> resp in responses {
</span><span>        stdout.</span><span style="color:#96b5b4;">write_all</span><span>(resp.response.</span><span style="color:#96b5b4;">as_bytes</span><span>()).await?;
</span><span>        stdout.</span><span style="color:#96b5b4;">flush</span><span>().await?;
</span><span>    }
</span><span>}
</span></code></pre>
<p>The streaming interface provides immediate feedback, making the tool feel responsive even when the model is working on complex queries.</p>
<h2 id="usage"><a class="zola-anchor" href="#usage" aria-label="Anchor link for: usage">Usage</a></h2>
<p>Let's walk through a typical interaction:</p>
<ol>
<li>
<p><strong>Initial Setup</strong>: I run <code>mdrag embed ~/vault</code> to process all my markdown files. This creates embeddings for each chunk and stores them in the SQLite database.</p>
</li>
<li>
<p><strong>Querying</strong>: When I want to find something, I run <code>mdrag search "how to debug git ssh issues"</code>. The system:</p>
<ul>
<li>Generates an embedding for my query</li>
<li>Finds the most similar chunks from my notes</li>
<li>Constructs a prompt with the query and context</li>
<li>Streams the AI-generated response</li>
</ul>
</li>
<li>
<p><strong>Getting Results</strong>: Instead of just showing me raw search results, I get a conversational answer that synthesizes information from multiple sources in my vault.</p>
</li>
</ol>
<p>For example, querying about Git SSH debugging would pull relevant snippets from my Git.md file and provide a comprehensive answer with the exact commands I've used before.</p>
<p>The system can also be modified to act as a chatbot, where users can ask follow-up questions and the system will use the context of the previous conversation for responses.</p>
<h2 id="local-first-approach"><a class="zola-anchor" href="#local-first-approach" aria-label="Anchor link for: local-first-approach">Local-First Approach</a></h2>
<p>Running everything locally has several advantages:</p>
<ul>
<li><strong>Privacy</strong>: My notes never leave my machine - this is crucial for vaults that contain sensitive information like work-related notes</li>
<li><strong>Cost</strong>: No API costs for embeddings or inference (other than electricity 😅)</li>
<li><strong>Reliability</strong>: Works offline and doesn't depend on external services</li>
</ul>
<p>The performance is surprisingly good. Initial embedding takes a few seconds, and subsequent queries are nearly instantaneous. The SQLite vector search is remarkably fast, and the 1B parameter model provides good results without requiring expensive hardware.</p>

        </section>
    </article>
</main>



        
            
        

        
    </div>
</body>

</html>
